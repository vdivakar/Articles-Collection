# Articles-Collection
Collecting worthy articles along the way...


[Papers-Stack](https://vdivakar.github.io/Papers-Stack/)

## ML Fundamentals
1. [A Gentle Introduction to Maximum Likelihood Estimation for Machine Learning](https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/)
2. [Discriminative vs Generative models - using Classification approach](https://mlwhiz.com/blog/2019/09/23/generative_approach_to_classification/?utm_campaign=a-generative-approach-to-classification&utm_medium=social_link&utm_source=missinglettr-linkedin)


## Deep Learning

### VAE
1. [Variational Autoencoders - Intuition, Statistical motivation, Latent space, a generative model](https://www.jeremyjordan.me/variational-autoencoders/)‚≠ê
2. [Variational Methods in Deep Learning](https://towardsdatascience.com/variational-methods-in-deep-learning-cad00c0ea018)
3. [Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)

### GRU (gated recurrent units)
1. [Coursera video, simplified](https://www.coursera.org/lecture/nlp-sequence-models/gated-recurrent-unit-gru-agZiL)
2. [Medium - Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
3. [d2l.ai - Chp 9.1 GRU](https://d2l.ai/chapter_recurrent-modern/gru.html)

### LSTM
1. [Colah's blog - Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. [Coursera video](https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm) 

### Attention & Transformer
1. Coursera - [Attention](https://www.coursera.org/learn/nlp-sequence-models/lecture/lSwVa/attention-model), [Tranformer](https://www.coursera.org/learn/nlp-sequence-models/lecture/YKatU/transformer-network-intuition), [Self-Attention](https://www.coursera.org/learn/nlp-sequence-models/lecture/lsvRK/self-attention), [Multi-Head Attention](https://www.coursera.org/learn/nlp-sequence-models/lecture/jsV2q/multi-head-attention), [Transformer n/w architecture](https://www.coursera.org/learn/nlp-sequence-models/lecture/Kf5Y3/transformer-network)
2. YouTube; CodeEmporium - [Transformer Neural Networks Explained! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps)
